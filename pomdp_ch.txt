---------概率机器人--------------------

现在让我们将焦点切换到部分可观的问题。在马尔科夫决策过程中只解决行动中的不确定性，但他们假设世界的状态可以有把握地确定。对于完全可观马尔可夫决策过程，我们设计了一个数值迭代算法用于控制随机域中的机器人。现在，我们将有兴趣在更一般的情况，其中的状态不是完全可观的。缺乏观测性是指机器人只能估计一个P后验分布在可能的世界状态，我们称之为belief。此设置也称为部分可观马尔可夫决策过程，或POMDPs。
在本章的其余部分要解决的核心问题是：对于POMDPs我们能否制定规划和控制算法？答案是肯定的，但具有前提。寻求最优策略的算法仅存在于有限的世界，其中状态空间，动作空间，观测空间，以及计划向量T均为有限的。不幸的是，这些精确的方法的计算是十分复杂的。对于更有趣的连续的情况下，最有名的算法都是近似的。
在MDPs中的中心更新方程如下
15。14
在POMDPs中，我们应用非常接近的想法，然而，状态并不是可观的。我们只可以得到states 的后验分布belief state b。因此对于POMDPs的计算方程如下
16.2
并且我们使用这个方程来生成控制
16.3
不幸的是，在belief空间计算价值要比在state空间复杂的多。belief是一个概率分布，因此，在POMDPs中的价值C_T是有关概率分布的函数，这就是问题所在。如果状态空间是有限的，由于belief space是所有状态空间的分布，所以belief state就是连续的。对于连续状态空间的问题，这个情况更加复杂，在这种情况下，belief space是一个无线维的连续体。除此之外，公式。。和公式。。对于所有的beliefs b进行积分。由于belief space的复杂度，积分并不容易精确计算或者有效地近似，
幸运的是，对于有限的情况，存在确切的解法。
本章安排如下，
第二节，讨论了一种更加普遍的同时考虑两种不确定性：动作不确定性和观测不确定性的调度算法。这种算法可以被应用于belief space的表示，在这种算法下的框架称作Partially Obeservable Markov Decision Processes(POMDPs)。这种算法可以估计不确定性，高效的获得信息，并且可以提高精确度在增加计算复杂度的情况下。

16.3 有限环境





在上一节得到的值迭代算法在有限的世界POMDPs。具体地，该算法要求各州，观测，和行动有限数，它也需要一个有限​​时域T。
[...]
仍有（16.35）条款需要进一步考虑。首先，眼前的收益函数C迄今只被定义过的状态。在这里，我们需要计算它的belief 分布。的belief B中的收益仅仅是每状态回报中c的belief B（S）（S）的期望：C（B）=的Eb [C] =？ C（S）B（S）DS
（16.69）
这似乎有点儿奇怪的回报依赖于机器人的belief 。然而，机器人的belief 表示真实后超过世界状态，这证明了预期。
第二，我们需要推导出概率P的表达式（邻？|的a，b），这是观测分布O 3在分布式状态执行后B人们可能接受
按B。这个推导式是一个简单的数学练习。尤其是，让我们先通过集成状态s？在该观察2 O由：
类P（o |？A，B）=？类P（o |？A，B，S）P（S |？A，B）DS？ （16.70）
因此，S？指动作执行后的状态。像往常一样，我们利用马尔科夫特性来简化这个表达式
=？类P（o |？S）P（S |？A，B）DS？ （16.71）
如果我们现在采取行动执行前通过整合状态s：
=？类P（o |？的？）？ P（S |？A，B，S）P（S | A，B）DS DS？
（16.72）
我们得到了简化说法=？类P（o |？的？）？ P（S |？A，S）B（S）DS DS？
（16.73）
有了必要的表达，我们现在可以返回到如何在belief 空间进行值迭代，并给出一个具体的算法，原来的问题。在partic- ular，让我们代替（16.73）回（16.34），这导致以下递归
在belief 的价值空间函数的定义：
CT（B）=最大
？ ?? CT-1（B（B，A，O 2））+？ C（S'）B（B，A，O 2）（S？）?? DS
一
？类P（o |？的？）？ P（S |？A，S）B（S）DS DS？怎么办？
（16.74）
这种递归定义定义最优值函数的任何有限的视距T的最优值函数的无限视野由贝尔曼方程chacterized：
C∞（二）=最大
？ ??C∞（B（B，A，O 2））+？ C（S'）B（B，A，O 2）（S？）?? DS
一
？类P（o |？的？）？ P（S |？A，S）B（S）DS DS？怎么办？
（16.75）由于在MDP情况下，值迭代C∞（b）使用一个函数C由repet-近似于
itively应用此更新方程。因此，中心值迭代算法如下：
C（B）← - 马克斯？ ？
C（B（B，A，O 2））+？ C（S'）B（B，A，O 2）（S？）?? DS
一
？类P（o |？的？）？ P（S |？A，S）B（S）DS DS？怎么办？
（16.76）
此更新方程推广价值迭代belief 空间。尤其是，它亲志愿组织的方式来备份值使用熟悉运动模型和熟悉的感知模型belief 间隔之中。更新公式是相当直观：通过最大化在所有行动获得的belief B的值。要确定的belief 下b执行行动的价值，公式（16.76）建议考虑所有转播servations O 2。一个这样的观察是固定的，后部belief 经由贝叶斯滤波计算，并且相应的值被计算。因此，现在缺少的是一个计算测量邻的概率？在公式（16.76），通过积分在所有​​状态s，所有后续状态s ?,和称重的belief 下b与钼和灰模型P（S由对应概率这些状态的组合得到的本probabilist |？A，S ）。
1：
2：3：
4：5：6：7：
8：9：10：POMDP算法（T）：对于所有的B做C（B）= 0 repeatfor直到
CON
v
呃
所有
b
办
gence C * =-∞所有人的做CA =？ ？行动
C（B（B，A，O 2））+？ C（S'）B（B，A，O 2）（S？）?? DS ？类P（o |？的？）？ P（S |？A，S）B（S）DS DS？怎么办？
如果CA> C * C * =钙11：12：C（B）= C *回复C
表16.2的一般的POMDP算法具有有限地平线T，其中，是如文中所指定的功能为B（b，一，邻？）。这个版本开放留下来的价值如何运作
C被表示，以及如何在步骤8中的积分被计算。
16.3.1总POMDP算法
表16.2描述了一般算法的belief 空间值迭代。这个算法仅仅是在原则算法不能在数字计算机实现的，因为它需要集成在无限空间，以及缺乏规范的值如何功能
C的表示。该算法接受规划水平
T作为输入，它被假定为有限。由于在价值IT-关合作基于状态的版本中，该值函数由0（行表16.2 2和3）初始化。线6
经过11确定最佳的行动belief 状态B相对值功能
C的中央更新，在第8行，相当于最大化的参数中

方程（16.76）。
最重要的剩下的问题涉及到C的性质进行价值
在迭代belief 空间，我们需要一种方法来表示了belief 价值的功能。belief 是概率分布。因此，我们需要为赋值概率分布的方法。很不幸，是一个很难的问题，缺乏一个通用的解决方案。如上注意到，在连续状态空间belief 空间POS-SESS无限多的维度，这表明价值函数定义在无限维空间。即使是有限状态空间，价值函数定义在一个连续体，使得它值得怀疑至于是否最优值函数可以在数字计算机上表示。然而，事实证明，价值功能可以精确计算的有限问题的重要的特殊情况，特别是对问题的有限状态，动作和观察空间，并与有限规划水平。确切的这样的算法将被给予在下一节。
16.4 MONTE CARLO逼近
现在让我们研究一个具体算法近似于普通POMDP算法，并已显示出希望在实际应用中。基本的想法是使用颗粒近似的belief 状态，使用particule滤波器算法calcu- latingbelief 。 Particule过滤器已经广泛在本书的不同章节进行讨论。他们在数学推导中第4.2.1章。
16.4.1蒙特卡洛备份
为完整起见，让我们简单地看一下基本更新方程。最初，N个随机样本从初始置信度分布B（0）绘制。然后，在时间t时，产生一组新的加权颗粒。
表16.3显示，接受一个行动，观测O的粒子滤波算法的变体？和belief B作输入。然后，它转换的belief b分成一个新的belief 。这种算法实现该函数为B（b，一，邻？），假定belief 状态由加权集颗粒的代表。它是从平凡的粒子滤波得到表4.3这本书的4.3页。见第4.2.1章进行更详细的解释。
手持为B（b，一，邻？），则更新方程可使用的直FOWARD蒙特卡洛algorihm来实现。召回belief 空间的价值函数被定义为1：2：
3：4：5：6：7：
8：9：
粒子滤波2（B，A，O 2）：
B' =∅
做样品
ñ
采样时间：S，P？从根据p1的B，。 。 。 ，PN在B S' 〜P（S |？A，S）
W' = P（O |？S）
正常化
添加
？的？，W？
到B？
回报
均为w？ ∈B' B'权重
表16.3变异的粒子滤波算法，它接受一个动作一个和观测O？作为输入。
以下recusive公式：
CT（B）=最大
？ ?? CT变体？和belief B作输入。然后，它转换的belief b分成一个新的belief 。这种算法实现该函数为B（b，一，邻？），假定belief ç持为B（b，一，邻？），则更新方程可使用的直FOWARD蒙特卡洛algorihm来实现。召回belief 空间的价值函数被定义为1：2：
3：4：5：6：7：
8：9：
粒子滤波2（B，A，O 2）：
B' =∅
做样品
ñ
采样时间：S，P？从根据p1的B，。 。 。 ）=»加
？的？，W？
到B？
回报
均为w？ ∈B' B'权重
表16.3变异的粒子滤波算法，它接受一个动作一个和观测O？作为输入。
以下recusive公式：
CT（B）=最大
？ ?? CT变体？和belief B作输入。焳MCPOMDP外环（二）：对于所有的B做C（B）= 0 repeatfor直到
所有CON
v呃b gence收益C（B）= MCPOMDP内环（B）C
1：
2：3：
4：5：6：
7：8：9：
10：11：12：
算法MCPOMDP外环（二）：对于所有的B做C（B）= 0 repeatfor直到
所有CON
v呃b gence收益C（B）= MCPOMDP内环（B）C算法MCPOMDP蚀变外环（二）：对于所有的B做C（B）= 0的重复，直到骗子v ER
B = P（S0）
gence
S〜P（S0）
做，直到审判0伏尔
-C（B）= MCPOMDP内环（二）
如果
兰特（0,1）> 0.1
否则选择= argmaxa C（B，A）一个随机13：14：15：
DRAW¯¯S' 〜P（S |？A，S）返回小号= S？ Ç

16.6摘要
在本节中，我们介绍了三种主要的算法下的不确定性动作选择。
马尔可夫决策过程（MDP）的框架解决在行动的结果是随机的情况下采取行动的选择问题，但机器人知道它的绝对肯定的状态。
值迭代通过计算状态上的价值函数解决离散的MDP。由贪婪地价值最大化选择行为导致最佳的行动选择。价值功能的一个关键特点是，它们引起了定义在整个状态空间的行动选择策略。不管是哪一个动作的结果是，机器人知道该怎么做。
的部分可观测马尔可夫决策过程（POMDPs）的更广泛的框架解决不完善传感器行动选择的问题。 POMDP算法必须立足于belief 的状态决定。解决方案的权衡信息收集（勘探）和剥削。
值迭代也可以应用于解决POMDPs。在世界有有限个状态，动作和观察，价值函数是在belief 空间的参数分段线性。然而，在最坏的情况下计算的价值函数是在规划期内双倍指数。
最后，我们介绍了增强马尔可夫决策过程（AMDP）框架，它融合的MDP和POMDPs。 AMDPs代表机器人的不确定性由低维统计（例如，熵），开门艾菲说考虑一些国有estima-化的不确定性cient规划算法。
值迭代也适用于AMDPS.Within移动机器人导航，将所得的算法被称为沿海导航，因为它导航机器人中，最大限度地减少在本地化的预期的不确定性的方法。
