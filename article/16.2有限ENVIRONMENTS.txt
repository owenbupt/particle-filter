16.2有限环境
下面考虑一种十分重要的特殊情况，有限环境。在这里，我们假设我们有一个有限状态空间，在每个状态具有有限的动作，有限数量的观测，和有限计划时域.在这些情况下，最优值函数可以被精确计算，也即具有最优策略。然而这并不是对所有情况都很明显，即使状态空间是有限的，事实上可以有无限多的可能的belief。然而，在这一点，下面我们建立一个众所周知的结果，最优值函数是凸的并且由有限多个线性部分组成。因此，即使该值函数被定义在一个连续体，它可以在计算机中以浮点数的精度来表示。
16.2.1说明性的例子
一个有限的世界的一个例子示于图16.1。这个具体的例子极端简单，其唯一的作用是在讨论一般的解决方案前帮助熟悉POMDPs的关键问题。我们注意到，图16.1具有四种状态，标记S1至S4中，两个动作，a1和a2，和两个观察，标记O1和O2。初始状态是随机抽取来自两个顶端状态，s1和s2。现在机器人给出一个选择：执动作作A1，这将具有高概率（但不总是）瞬间移动到相应的其他状态。另一种可选择方案，它可以执动作作A2，这导致在这两个底状态，S3或S4的概率中的一个的转变所示。在S3中，机器人将收到一个大的正收益，而在S4的收益为负。这两个state是终端的状态，也就是说，一旦进入任务就结束了。因此，机器人的目标是在状态S1时要执动作作A2。如果机器人知道它是在什么状态，执行任务是非常简单：简单地套用动作A1，直到状态是S1，然后应用动作A2。然而，机器人不知道其状态。相反，它可以感知两个观测O1和O2。不幸的是，这两个观测可能在这两种状态，虽然观察一比其他的概率不同，这取决于什么状态的机器人是由于机器人并不知道它的初始状态是S1或S2，它必须仔细跟踪过去的观测来计算它的belief。因此，关键的策略问题是：什么时候应该机器人尝试动作A2？具有多大的置信度去执动作作a2，需要多长时间来尝试这个动作？这些和其他问题将在下面进一步描述。
本节的目标是开发一种在有限状态和有限时域T的情况下来计算最优值函数的算法。让我们表示用T来表示计划时域和用S1,...,SN表示状态。利用状态空间的有限，

图16.1有限状态环境，用来说明价值迭代的belief空间。

我们注意到，belief B（S）由n个概率给出
圆周率= B（S= SI）（16.4）
其中i=1，。 。 。 ，N。
belief必须满足的条件
PI≥0
（16.5）
因为最后一个条件的，B（S）可通过n-1个参数p1,...,p_n而不是n个参数指定。剩余的概率的pn可以根据如下公式计算：
（16.6）。
因此，如果状态空间是有限的，并且大小为n的话，则belief是一个（N-1）维时域。
在我们的图16.1示出的例子中，可能出现的是由三个数值的概率值指定的belief，因为有四个状态。然而，操作A2将状态{S1，S2}和状态{S3，S4}分开。因此，该机器人可能遇到的唯一的不确定性是状态s1和s2或者状态S3和S4之间的的混淆。两者均可以由一个单一的数值的概率值来表示，由此，belief状态的唯一连续成分是一维的。这使得这个例子方便绘制值的函数。
现在，让我们专注于一个问题，我们是否可以计算出最优值函数，准确地在有限的领域最优策略。如果没有，我们可能会被迫以接近它。起初，人们可能会得出这样的结论计算最优值函数是不可能的，由于这一事实，即belief空间是连续的。但是，我们观察到的有限的世界，值函数有一个特殊的形状：它是由有限多个线性件。这使得能够计算在有限时间内的最优值函数。我们还注意到，价值函数是凸和连续这后两种特性也适用于最优值函数在连续状态空间和无限的规划视野。
我们开始的belief状态B的直接回报我们的考虑。回想一下，b的收益是由下概率分布B的收益C中的给定期望：
C（二）=？ C（S）B（S）DS（16.7）
使用使得b唯一地由概率p1指定的事实。 。 。 ，PN，我们可以写C（B）= C（P1，。，PN）=
ñ
？
C（SI）PI
I = 1
（16.8），它确实是线性p1中，。 。 。 ，PN。
图16.2期望收益C（B）为belief参数P3的功能，假设机器人无论是在状态S3或S4。
有趣的是绘制函数c（b）在置信度分布在两种状态S3和S4在我们的例子中，只有国家与非零回报。在状态S3回报
是100，而在状态S4的回报是-100。图16.2a示出了函数c（b）为相信通过？0,0，第3页，1-P3限定的子空间？这是一种belief的空间，放置在美国S3和S4所有的可能性。这里的期望C（b）的绘制为
函数的概率P3的。显然，如果P3 =0时，环境状态是S4，并回报将是C（S4）= -100。另一方面，如果P3 =1，环境的
状态是S3和回报将是C（S3）= 100。在此期间，期望是线性的，从而导致在图16.2a所示的曲线图。
这种考虑使我们能够计算价值函数规划时域T = 1。从现在开始，我们将只考虑belief空间的地方的子集
在两种状态S1和S2所有的可能性。这种belief空间是由一个单一的参数参数化，p1的，因为P2 = 1 - P1和P3 = P4 = 0的值函数C1（二，a1）中是恒定的零动作A1：
C1（二，A1）= 0（16.9）
因为无论机器人的真实状态，动作A1不会导致它的状态，使得它收到非零的回报。该值函数C1（B，A1）如果绘制如图16.2b。图片变得更有趣动作A2。如果环境的状态是S1，这个动作将导致90％的几率状态S3，其中机器人将收到的100的收益有10％的概率会结束在状态S4，在那里它会回报
-100。因此，在状态S1的预期收益为0.9·100 + 0.1·（-100）= 80。类似的说法，状态S2的预期收益为-80。在这两者之间，期望是线性的，产生的价值函数
C1（B，A2）=γ（80p1 -80p2）= 72p1 -72p2（16.10）
这里我们使用的折现因子γ= 0.9。这个函数表示在图16.2c，该类型的belief？P1，1-P1，0，0℃。
那么，什么是正确的动作选择策略？继最大化预计这将对收益的理由，最好的动作取决于我们目前的belief，假设它准确
反映了我们对现实世界的知识。如果概率P1≥0.5，最佳动作将A2，因为我们期望正回报。对于值小于0.5，最佳动作将A1，因为它避免了与动作A2相关的负面预期收益。对应的价值功能的动作特定值功能最大：
C1（B）=最大为C1（B，A）
=最大值{0; 72p1-72p2}（16.11）
该值函数和动作选择了相应的策略由图16.2d，从而最大程度地说明了两个线性成分的固体图所示
由虚线。我们注意到，这个数值函数不是线性的任何更长的时间。相反，它是分段线性和凸出的。非线性产生的事实，不同的操作是最佳的belief空间的不同部分。
从图16.2d的价值功能，我们可以得出结论，beliefP1 <0.5是没有办法获得的收益大于0？当然，答案是否定的。该
值函数C1（b）是仅最适合于时域T = 1。对于较大的视野，也能够先执动作作A1，接着动作A2。执动作作A1具有两个有益效果：首先，它有助于我们估计当前状态更好，由于这样的事实，在我们可以感知，并且第二，以高概率从s1至s2的，或反之亦然改变状态。因此，一个良好的策略可能会执行的动​​作A1，直到我们有理由相信，机器人的状态是S1。然后，机器人应执行的动作A2。
为了使这个比较正规的，让我们得出的最优值函数的时域T = 2，假设我们执行的动作A1。然后，两件事情可能发生：我们要么遵守O1 O2或。让我们先假设我们观察到O1。那么新的belief状态可以使用贝叶斯滤波器来计算：
2个P？ 1 =η1P（O 1？|？第1条）
？
P（S 1 |？A1，SI）PI
I = 1
=η10.7（0.1p1 + 0.8p2）=η1（0.07p1 + 0.56p2）
（16.12）
其中，η1是贝叶斯法则的正规化。同样，我们得到的是S2后概率：
2 p? 2 = η1 P(o? 1|s? 2)
?
P(s? 2|a1, si) pi
i=1
= η1 0.4(0.9p1 +0.2p2) = η1 (0.36p1 +0.08p2)
(16.13) Since we know that p? 1 + p? 2 = 1—after all, when executing action a1 the state will
either be s1 or s2—we obtain for η1: η1 =
1 0.07p1 +0.56p2 +0.36p1 +0.08p2 = 1 0.43p1 +0.64p2 (16.14)
我们也注意到，变量η1是一个有用的概率：这是执行的动作A1（无论后状态的）后观察O1的概率。
我们现在有表情刻画因为我们执行AC-后belief
A1化观察O1。那么，什么是这个国家的belief价值？答案是转播tained通过插入新的belief到价值函数C1（b）的公式定义（16.11），和γ贴现的结果：
C2（B，A1，01）=γ最大值{0; 72P？ 1 -72p？ 2}
= 0.9最大值{0; 72η1（0.36p1 + 0.08p2）-72η1（0.43p1 + 0.64p2）} = 0.9最大值{0; 72η1（0.36p1 + 0.08p2 -0.43p1 -0.64p2）} = 0.9最大值{0; 72η1（-0.07p1 -0.56p2）} = 0.9最大值{0; η1（-5.04p1 -40.32p2）}
（16.15）
我们现在移动的因素η1出来的最大化和移动0.9里面，并取得：C2（B，A1，01）=η1最大值{0; -4.563p1 -36.288p2}
（16.16），其是一个分段线性的，凸的函数的beliefb的参数。
推导的执行动作A1后观察O2是完全类似。特别是，我们得到的后验
p?
1 = η2 0.3(0.1p1 +0.8p2) = η2 (0.03p1 +0.24p2)
p?
2 = η2 0.6(0.9p1 +0.2p2) = η2 (0.54p1 +0.12p2)
with the normalizer η2 =
1 0.57p1 +0.36p2
The corresponding value is C2(b, a1, o1) = γ max{0 ; 72p?
1 −72p? 2} = η2 max{0 ; −33.048p1 +7.776p2} (16.17) (16.18) (16.19)
顺便说一句，我们也注意到，η-1 1 +η-1 2 = 1。这直接遵循这样的事实
在贝叶斯正规化过滤是观察概率ηI=
1 P（O I |？A1，B）（16.20）和事实恰好有两种可能的观测我们的例子中，O1和O2。
现在让我们计算值C2（二，a1）中，这是在执动作作A1的预期值。显然，该值是值C2（二，A1，01）和C2（二，A1，O2），通过实际观察O1和O2分别的概率加权的混合物。放入数学符号，我们有
C2（二，A1）= 2
？
C2（B，A1，OI）P（OI | A1，B）
I = 1
（16.21）
术语C 2（二，A1，OI）已经如上所定义。概率P（OI | A1，B）执动作作A1之后观察OI是η-1
我。因此，我们有所有成分
计算所需值C2（二，a1）中：C2（二，A1）=η-1
1η1最大值{0; -4.563p1 -36.288p2} +η-1
2η2最大值{0; -33.048p1 + 7.776p2}（16.22）= MAX {0; -4.563p1 -36.288p2} +最大{0; -33.048p1 + 7.776p2}
该表达式可以被重新表示为四个线性函数的最大值：C2（二，A1）=最大{0; -4.563p1-36.288p2;
（16.23）-33.048p1+7.776p2; -37.611p1-28.512p2}
图16.2e表示这四个线性函数的belief间隔p1，1 - P 2，0，0℃。该值函数C2（B，A1）是这四个线性函数的最大值。由于是
容易被看到的那样，二者的这些功能是足以定义最大;其他
2顷在整个频谱较小。这使我们能够改写的C2（二，a1）中为最大值的仅有的两个方面，而不是四个：C2（二，A1）=最大{0; -33.048p1+7.776p2}
（16.24）
最后，我们要确定的值C2（b）中，这是下面两个条件中的最大值：
C2（B）=最大{C2（二，a1）中，C 2（二，A2）}（16.25）
正如很容易地验证，则第二项的最大化，C2（二，a2）中，是完全与上述相同的用于规划时域T = 1：
C2（二，A2）= C1（二，A2）= 72p1 -72p2
因此，我们获得（16.24）和（16.26）：
（16.26）
C2（B）=最大{0; -33.048p1 + 7.776p2; 72p1 -72p2}
（16.27）这个函数是最优值函数规划时域T.图？图
C2（b）和其组成部分的belief子空间P1，1 - P 2，0，0℃。正如很容易看到的，函数是分段线性和凸出的。特别是，它包括三个
线性件。对于在最左边的两个片belief（​​即P1 <0.5），A1是最佳的动作。对于P1 = 0.5，这两个动作都是一样的好。对应于最右边的线性片的belief，即用P1> 0.5的belief，有最佳的动作A2。如果a1为最优函数，下一个动作取决于在belief空间的初始点和在观察。
价值函数C 2（b）是仅最适合于时域2。但是，我们的分析示出了几个重要的观点。
首先，最优值函数的任何有限的视距是连续的，分段线性，凸。每个线性一块对应于一个不同的操作选择在将来某个时候，或者，可以进行不同的观察。价值函数的凸性表示相当直观的观察，即明知是AL-方面都优于不知道。给定两个belief状态B和B?,的belief状态的混合值大于或等于该混合belief状态的值，对于某些混合参数β0≤β≤1：
βC（二）+（1-β）C（二？）≥-C（βB+（1-β）B'）（16.28）
二，直线条的数量可以极大地增长，特别是如果一个人不注重线性函数变得过时。在我们的玩具例子中，两个出的四个直线约束限定的C2（二，a1）的是没有必要。 EF-ficiently实施POMDPs的“诀窍”在于早识别过时线性函数
如可能，从而使计算的价值函数时，没有计算被浪费。不幸的是，即使我们仔细排除一切不必要的线性约束，线性函数的num- BER仍然可以增长得非常迅速。这对对有限POMDPs的精确值迭代解决方案固有的扩展限制。

16.2.2belief空间中的价值迭代
在上一节展示来如何计算有限的价值函数。现在让我们回到在belief空间上价值迭代的一般问题。特别是，在介绍这一章中，我们指出了价值函数，我们简要地重申这里的基本更新方程：
（16.29）
在这个和下面的章节中，我们将开发一个可在数字计算机上实现的通用的在belief空间计算价值迭代的算法。首先，我们注意到，公式（16.2）提出对所有的belief进行积分，其中每一个belief是一个概率分布。如果状态空间是有限的，并且状态数为n，所有的概率分布的空间是具有维数n-1的连续体。我们注意到，为了指定在n个离散事件的概率分布n-1个数值是必需的（第n参数可以省略，因为概率加起来为1）。如果状态空间是连续的，belief空间具有无限多的维度。因此，对所有的belief的积分似乎是计算艰巨的任务。然而，我们可以通过重新制定的问题，并对所有观测进行积分来代替。
让我们看看条件概率P（B|？A，B），其中规定了在分配后的belief B'给定一个belief b和动作a。如果只有b和a是已知的，该后验belief B'不是唯一的，而且P（B |？的a，b）是belief的一个真正的概率分布。但是，如果如果我们也知道测量Ò，执行动作时，后验B'是独特的，P（B|？的a，b）是一个point-mass分布的退化。从动作执行之前的beliefb，动作a，和随后的观测O 3，贝叶斯滤波器计算一个单一的准确的后验概率belief B'。因此，我们得出结论，只要我们知道O 3，对所有的belief的积分会退化。
这种洞察力可以通过重新表达P利用

（16.30）

其中，P（B |？的a，b，邻）是由贝叶斯滤波器计算的point-mass 分布将积分代入价值迭代的定义（16.2），我们得到
（16.31）
内积分
（16.32）
仅包含一个非零项。这其中b‘是由B，A和O的分布通过贝叶斯滤波器计算得到。让我们用B（B，A，邻？）来表示这个分布，那么有
（16.33）
我们注意到，正规化分母，P（O|？A，B），是值更新方程（16.31）的一个因素。因此，将B（B，A，O 2）代入（16.31）消除了这一量，从而导致了值迭代算法的递归描述：
（16.34）
这种形式是比在（16.2）更加方便，因为它只需要对所有可能的测量O 2，而不是所有可能的belief分布b？进行积分，这种转变是派生此一章所有值迭代算法的一个重要观点。事实上，它是含蓄地用在上面的例子中，在一个新的价值函数是由有限多个分段线性函数混合获得的。
下面，很容易根据积分获得最大化价值的动作。因此，我们会注意到（16.34）可以改写为下列两个等式：
（16.35）
（16.36）
在这里，CT（B，A）是时域T对于假设下一个动作a的belief b的函数。

16.2.3计算价值函数
在我们的例子中，最优值函数是分段线性和凸的。这种情况，对于一般的有限情况均满足。对于所有具有有限时域的有限POMDPs的最优值函数都是分段线性和凸的。分段线性度意味着价值函数CT由线性函数的集合表示。如果CT是线性的，它可以通过一组系数。。。CT来表示

（16.37）
其中，像往常一样，P1。 。 。 ，PN是一种belief分布的参数。在我们的示例中，分段线性和凸值函数CT（B）可以通过K个线性函数集合的最大值表示

（16.38）
其中，CT K，1，。 。 。 ，CT K，n分别表示第k个线性函数的参数，以及K表示线性片段的数量。一组有限的线性函数的最大值事实上是凸的分段线性的函数。

在值迭代，初始值函数由下式给出
C0 = 0（16.39）
我们注意到，这个数值函数是线性的，因此，它也是分段线性和凸。这种分配建立了递归值迭代算法的基础。
我们现在衍生除一个计算值函数CT（b）的递归方程。该公式假定上一步的值函数CT-1（b），由一个分段线性函数如上规定表示。作为派生的一部分，我们将假设CT-1（b）是分段线性和凸的，CT（b）是也分段线性和凸出的。下面将证明具有有限时域的值函数是分段线性和凸的。
方程（16.35）和（16.36）定义如下：
（16.40）
（16.41）
在有限的空间中，所有的积分可以通过有限和取代，我们得到：

（16.43）
belief B（？B，A，O）使用下面的表达式获得，该式为用和来替换（16.33）中的积分所得到：

B（？B，A，O）（S？）= P（O|？A，B）P（O|？S）？P（S|？A，S）B（S）1秒（16.44）

如果belief B由参数{p1，。。。 ，PN}表示，那么belief b’中的第i个参数由下式计算得到

（16.45）

为了计算价值函数CT（B，A），我们将现在的条件c（B（B，A，O 2））和CT-1（B（B，A，O 2） 中衍生出更多实用的表达式中），从公式（16.42）开始，具有参数B（B，A，O 2）={对？
1，。 。 。页？ N}，我们从。。得到了

（16.46）
代入（16.45），我们得到
（16.47）
由于P（邻？|的a，b）不依赖于i，后者变换是合法的。这种形式仍然包含一个难以计算的表达式：P（O| A，B？）。然而，在有限的情况下这个表达式可以被消掉。因此，它不必被任何进一步考虑。
在（16.42）中对量CT-1（B（B，A，O 2））的推导类似于上面C（B（B，A，O 2））。特别是，我们不得不用唯一的分段线性和凸的值函数CT-1取代直接回报函数c。然后可以得到

（16.48）
。如上所述，我们代入后验概率belief B（见（16.45）），并获得：

（16.49）
其中P（ω0| A，B）-1不依赖于i或K，故可移出求和和最大化部分，得到：
（16.50）
该表达式也是分段线性和凸的可能并不明显。然而，对该式进行整理得到下式：

（16.51）

在这种形式下，很容易验证，对于任何的固定值k，使其最大化的参数是确实线性的参数PJ。因而，这些线性段的最大值为分段线性和凸的。此外，线性段的数量是有限的。
现在让我们回到本节中的主要问题，即计算CT（B，A）和时域T的值函数。我们简要地重申方程（16.42）：
16.52）

让我们首先用公式（16.47）和（16.50）计算总和C（B（B，A，O）？）+ CT-1（B（B，A，O）？），：

(16.53) 
经过一些整理得到，
(16.54)

将这个表达式代入公式16.52
(16.55)
我们注意到，项P（邻|的a，b？）同时出现在在此表达式的分子和分母中从而抵消了，得到：
（16.56）

这个消除是仅对有限POMDP满足的。所需的价值函数通过对于所有动作最大化CT（B，A）：
CT（B）得到

（16.57）
很容易验证，在belief参数为P1，。 。 。 ，PN的情况CT（b）是确实分段线性和凸的。具体地，对于每个固定k（16.56）的大括​​号内的量是线性的。这些k个线性函数的最大值是一个分段线性函数，包括最多有K段。分段线性的凸函数的总和为再次分段线性和凸的。因此，因此对于所有观测Ø？的乘积的总和再次产生一个分段线性凸函数。最后，对于在方程（16.57）中的所有动作的最大值也是分段线性的凸函数。因此，我们的结论是在任何有限时域T，CT（b）是分段线性和凸出的。