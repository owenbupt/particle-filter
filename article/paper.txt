# 摘要
# Abstract
# 绪论
## 选题背景及研究意义
雷达自从其被发明那天开始就在人类的生产生活中占据了重要的位置。在科学技术日益进步的今天，无论是无人驾驶还是飞机巡航等方面都少不了雷达的身影。为了提高对目标的探测的准确性,以及在有多个目标的情况下实现低误差性,现代的雷达系统都具有多种工作模式对目标进行探测[1]。对于多个不同的模式或波束进行调度是一个很重要的问题，也即本选题的波束调度问题。对于该问题，一方面需要提高结果的准确性,另一方面又要减小传感器状态转变或者探测的花费。传感器调度问题一般为离散优化问题。对于该问题有很多种解决方法,如动态规划、贪心算法、枚举法，或者是本选题采用的Rollout算法。Rollout算法可以说是一个权衡计算复杂度与算法精确性之后的结果，具有重要的应用价值。因此，本课题将Rollout算法应用于雷达多波束调度问题，并与短视算法、动态规划算法进行比较分析，为工程应用提供指导和借鉴。
## 国内外研究现状
- 调度算法
1.精确性算法精确性算法用于得到问题的最优解。在求解整数规划问题中，精确性算法主要有 0-1 规划算法、动态规划算法、分支定界算法三个方向。0-1 规划问题是决策变量仅取值为 0 或 1 的问题，求解这类问题主要为隐枚举法(如分支定界法)，Patterson 和 Huber 首先使用 0-1 规划对 RCPSP 问题进行建模，然后使用定界法对问题进行求解，最后与枚举法进行对比。求解结果优于枚举法[6]。动态规划是将多阶段决策过程转化为一系列单阶段决策问题，Garruthers 和Battersby 最早将动态规划算法应用到资源受限项目调度的研究[7]，虽然求解结果较CPM 和 PERT 有较大的改进，但是仍然不能解决这类问题。分支定界算法在求解资源受限问题上，由于其计算效率和结果的优势被较多的学者研究。基本思想是：对满足当前约束的所有解空间进行搜索，将全部可行解空间分割成多个子集，再使用一定规则淘汰掉不可能产生最优解的子集，选择可能产生最优解的子集，来缩小搜索空间，从而找到最优解。Patterson 等提出基于紧前树的分支定界算法[8]，Demeulemeester 等通过引入延迟替代集来解决资源冲突[9]。使用精确性算法求解 RCPSP 问题比较适用于小规模问题，能在一定的时间内得到问题的精确解。但由于该问题是 NP-hard 问题，问题的求解时间和内存会随任务数目的增加而呈指数增长，当问题规模较大时，求解时间会太长，同时会出现内存不足的现象，无法得出调度结果。2.智能优化算法对于 RCPSP 中、大规模问题，无法使用精确性算法求解，通常使用智能优化算法来求解，可以在一个较短的时间内得到问题的次优解，下面具体讨论智能优化算法在项目调度中的应用。（1）遗传算法遗传算法是由生物学的进化规律演变而来的随机性搜索算法，Davis 首次将遗传算法应用到项目调度中[10]，解决了车间项目调度问题。Hartmann 使用任务列表编码[11]，提出了基于优化规则和随机性的遗传算法。刘士新以遗传算法为基础，设计一种基于定界策略的近似算法[12]，杨宏利结合任务存储的邻接矩阵，有效的解决编码中无效的任务调度现象[13]。（2）蚁群算法蚁群算法是以蚁群觅寻食物源最短路径为背景提出的智能优化算法，Dorigo 和Colorni 首先提出蚁群系统[14]，并将该算法首先成功的应用在旅行商问题。Merkle等将蚁群算法应用在项目调度中[15]，将蚂蚁走过的路径映射为调度方案，通过结合优先规则来选择任务。（3）粒子群算法粒子群算法是通过粒子间信息共享和协作来寻找最优解，Kennedy 等[16]在 1995年首先提出这种算法。Zhang[17]将该算法应用项目调度中，对基于任务列表和优先粒子两种情况，分别采用不同的进化策略求解。（4）模拟退火算法模拟退火算法是对局部搜索算法的改进[18]，Boctor 在 1996 年首次将该算法应用到 RCPSP 问题中，并将结果与禁忌算法相比较，结果显示该算法的实用性和有效性[19]。Cho 和 Kim 使用一个队列来表示一个调度方案，队列中的元素代表任务序号，通过模拟退火方法需找优先队列[20]。（5）禁忌算法禁忌算法通过对优化过程进行记录和选择，建立禁忌表，来避免陷入局部最优。Nonobe 和 lbarakiP 根据实际应用对 RCPSP 问题进行扩展，并使用禁忌算法启发式算法求解问题[21]。Atli 分别使用禁忌算法和 CPLEX 软件求解 RCPSP 问题，结果表明禁忌算法在求解时间上有着明显的优势，CPLEX软件仅适合求解小规模问题[22]。3.启发式算法启发式算法内容比较丰富，主要分为：基于优先规则、局部搜索技术和元启发式算法。其中基于优先规则的启发式算法应用较广泛[23]，它通过一定的启发规则快速得到问题的解，但不保证解的质量。基于优先规则的启发式算法主要由两个部分组成：进度生成机制和任务优先规则。进度生成机制是大多数启发式算法的核心部分，根据扩展方式的不同，分为以任务为阶段变量和以时间为阶段变量两个类型，其中以任务、时间为阶段变量分别称为串行进度生成机制(SSGS)和并行进度生成机制(PSGS)[2]。Hartmann 等[24]对两种不同机制下调度问题进行研究，结果表明，当项目中任务比较多，PSGS 较优，而任务比较少时，则SSGS较优。由于PSGS的是非延迟进度机制，搜索空间少于SSGS，当任务比较少，使用 SSGS 搜索解与精确性偏差小，当任务较多时，使用 PSGS 能很快搜索问题的解空间。任务优先规则用于在项目调度中赋予任务优先权，在进度生成机制时基于任务优先权分配任务开始时间和结束时间。文献[25]分别提出最短工期、最长工期、最多紧后任务、最多后续任务、最大秩序权重、最早开始时间和最早结束时间六种优先规则。刘士新等[26]对众多优先规则进行研究，结果表明使用优先规则求解的效果与项目的内部结构有关，需要根据项目的参数特点来选择一个或多个优先规则组合对问题求解。由于进度生成机制和优先规则的多样性，启发式方法可分为单次算法和多次算法。如果只使用一种进度生成机制和一种优先规则对项目进行调度，则称为单次算法；如果使用两种进度生成机制或多种优先规则对项目进行调度，则可以得到多个调度方案，并从中挑选较好的，则称为多次算法，多次算法调度结果优于单次调度算法[27]。
- POMDP 问题 
俄罗斯数学家马尔科夫于 1907 提出的马尔科夫决策过程，这类决策过程有一个共同的特性即为马尔科夫性，也称无后效性。它是指某阶段的状态一旦确定，则此后过程的演变不再受此前各状态的影响。具体地说，如果一个问题被划分各个阶段之后，后一个阶段中的状态只受前一阶段的影响，与其他状态没有关系，且是由前一阶段的状态通过状态转移方程得来的。MDP 是基于决策理论的规划方法中应用最多的模型。它描述了决策节点与环境相互作用的过程。
POMDP 引入了信念状态这一概念，在信念状态具有马尔科夫性，同时由于信念状态空间的连续n维性，使得 POMDP 的求解更加困难。但是，POMDP 是在MDP 基础上扩展的一个更一般化的模型，它能够很好地对环境、动作和观察的不确定性进行建模，相对于 MDP 更贴近于现实情况，具有更广泛的应用。所以，从POMDP 模型提出至今，一直都得到了各个领域学者们的广泛关注，先后产生了很多有用的技术，以解决不确定环境下的决策与规划问题。POMDP 算法研究现状。为了快速高效准确的求解 POMDP 问题，从最初的精确求解算法到后来的近似求解方法，先后出现了很多的求解算法。POMDP 的求解算法可以按照前向或后向分类，也可按照精确求解和近似求解分类。本文按照后者来进行分类介绍。1971 年 Sondik 在其博士论文中提出的一次合格算法拉开了精确求解 POMDP的序幕。随后出现了一系列的精确求解算法，例如 Monahan 的算法，线性支撑算法，目击算法，增量剪枝算法等。由于 POMDP 精确求解的困难性，以精确算法为基础，研究高效的近似算法将是解决 POMDP 问题的重要方向。文献[5]总结了一些近似算法，例如网格方法，有限历史方法，启发式方法，因子化信念状态分解方法，神经网络方法等。这些方法大部分都是 2000 年以前的文献中出现的。2000 年至今，POMDP 算法的研究仍在继续。下面按照时间顺序仅列举每年出现的比较重要的方法，基于点的 POMDP 求解方法 PBVI[6]于 2003 年提出，开启了基于点的 POMDP 方法的历程，至今基于点的方法都是求解 POMDP 相对最有效的方法；04 年提出的新的启发式搜索方法求解 POMDP[7]；05 年基于点的方法有了新的重要发展，提出了 Perseus[8]方法；06 年的混合 POMDP 算法[9]；07 年的AEMS[10]，Forward search value iteration[11]，基于点策略迭代[12]等；08 年基于点的另一个重要方法被提出，即 SARSOP[13]，同时文献[14]提出的在线求解方法也是一个重要的方法；2009 年文献[15]通过对信念空间分解或压缩来求解 POMDP；2010年文献[16]使用局部逼近的方法求解高维连续 POMDP 问题，文献[17]用 KaczmarzIterative Method 进行 POMDP 值函数的剪枝，以及通过对信念空间的聚类或压缩来实现 POMDP 的快速求解[18]，另外还有 Density Projection[19]方法，在线方法 AnOnline Algorithm for Constrained POMDPs[20]等等。随着 POMDP 求解方法的不断发展，其应用研究也得到了广泛关注。POMDP越来越被应用于重要领域，文献[21]总结了 1998 年之前 POMDP 的一些应用研究。但是，此后随着研究的不断深入，POMDP 的应用得到了更加迅速地发展，主要包括以下几个领域。机器人领域一直是POMDP的一个重要应用领域，如文献[22]将Layered POMDP应用到基于视觉的移动机器人的场景分析；文献[23, 24]将混合可观察的 POMDP 应用到不确定环境下机器人任务规划，用基于点的方法求解因子化的混合可观察POMDP 模型；多机器人协同[25, 26]；机器人的视觉行动[27]。在导航方面的应用，文献[28]就是利用 POMDP 解决不确定环境中的导航问题；文献[29]通过简化状态空间的方法应用于机器人导航。动作规划（Motion Planning）：Yanzhu Du 等人在文献[30, 31]利用 POMDP 进行UAV 动作规划。POMDP 在其他一些领域也得到了广泛应用，文献[32]利用 POMDP 建立智能教学系统；辅助能源管理[33, 34]；口语对话系统[35]；老年人辅助系统[36]等。从 POMDP 的发展过程总结可见，十多年前最好的 POMDP 算法也只能够求解仅有几十个状态的 POMDP 问题。五年前，基于点的算法则能够求解大概具有 900个状态的 POMDP 问题，这可以称得上是 POMDP 求解规模上的一个很大进步。现在，具有几百个状态的POMDP问题可以在很短的时间内得到解决[23]。正是POMDP算法的不断发展，带来了其更加广泛的应用。但 POMDP 求解尚存在一些基本问题需要解决：虽然 POMDP 范式比较广义，但仍需要进一步的扩展研究；虽然 POMDP 的新算法层出不穷，但有效地解决大规模实际 POMDP 问题的方法还有待探索，等等。与此同时，随着新一代智能系统的发展，更大规模的规划问题将越来越普遍，给 POMDP 提出了一些新的挑战，因此，本文对在随机迭代过程的基础上进行进一步探索，希望本文所作的工作能够为大规模 POMDP 问题的求解做出一定的贡献，并能在动态不确定的、结构型关系的智能系统中得到很好的应用。
目前关于部分可观察马尔可夫决策过程(Partially Observable Markov Decision Processes ,POMDP) 的研究十分活跃 , 人们提出了一些解决POMDP 问题的算法 . 例如 : 策略迭代算法 [1] (PolicyIteration ,PI) , 梯度法 [2] ( Gradient Ascent , GA) , 干枝界限法 (Branch and Bound ,B &B) 和随机局部查找法 (Stochastic Local Search ,SLS) 等 . PI 算法需求解线性方程组 , 其规模是一个指数函数 ; GA 算法难以求解整体最优 ;B &B 算法和 SLS 算法受限于计算时间而难以应用到大规模问题 . Witness 算法 [3] 、 Incremental Pruning 算法和 Point 2 based 算法 [4] 虽然可以解决有限时间问题 , 但对于无限时间问题却难以求出最优策略和最优值函数 [5] .
- 粒子滤波算法
国外对粒子滤波的研究起步较早。早在二十世纪五十年代Hammersley等人就提出一种被称为Sequential Important Sampling(SIS)的方法。到了六十年代后期Handschi等将SIS法应用于控制领域。七十年代各个领域的学者继续沿着SIS的思路研究，在这以后一系列改进的SIS算法[1  ,2 ]相继出现。但是，SIS算法容易导致粒子退化现象(Particle Degeneracy)，影响了它在实际中的推广应用。直到1993 年， Gordon 等人提出了采样重要性重采样算法 (Sampling importanceResampling, SIR)[ 3]，才基本解决了粒子退化问题。从此，粒子滤波又被广泛关注，并取得了重大进展，提出了一些重要的SIR算法。如，多项式重采样算法(Multinomial Resampling)[4 ]，分层重采样算法(Stratified Resampling)[5 ]，残差重采样算法(Residual Resampling)[6 ]，系统重采样算法(Systematic Resampling)[7 ]等，文献[8]详细介绍了重采样及相关算法，文献[9]对重采样模式进行了比较。由于SIR算法是对权值较大的粒子进行多重复制，造成粒子多样性损失，最终仍然可能出现粒子退化问题。文献[10~14]对重采样算法进行了分析研究，做了一些改进。国内对粒子滤波的研究开始较晚。但是许多大学和科研院所都对其十分关注，并进行了相关应用与理论研究。在粒子滤波理论方面，文献[15~17]介绍了粒子滤波基本理论和国内外最新进展；在粒子滤波的改进方面，文献[18]提出了一种使用非等权值粒子的确定性粒子滤波算法，文献[19] 提出一种扩展卡尔曼粒子滤波算法的修正方法；在粒子滤波的应用方面，文献[20]将粒子滤波应用于轮廓线跟踪，文献[21]应用于行人跟踪，文献[22]用粒子滤波在闪烁噪声环境下进行目标跟踪，文献[23]讨论粒子滤波在混合状态与参数估计中的应用，文献[24，25]将粒子滤波用于被动定位跟踪。目前国内对粒子滤波的研究水平与国外相比尚有较大差距，但各相关领域的学者都在孜孜不倦的研究探讨，相信会逐渐接近并超过国外的研究水平。
## 研究目标和内容
1. 基于POMDP的多目标跟踪雷达波束调度建模
2. 基于短视策略的多目标跟踪雷达波束调度方法
3. 基于动态规划的多目标跟踪雷达波束调度方法
4. 基于Rollout的目标跟踪雷达波束调度算法

## 技术路线
1. 对国内外相关经典文献进行调研、分析、整理；
2. 分析多目标跟踪雷达波束调度的状态空间、行动空间、量测空间、状态转移函数与观测函数，将目标状态后验概率分布作为置信状态，在雷达波束使用代价约束下，以跟踪精度作为待优化目标，建立基于POMDP的波束调度模型；
3. 假设目标为匀速直线运动，量测在雷达坐标系下，无杂波，检测概率不为1，采用粒子滤波估计目标状态，采用概率数据关联方法解决量测来源不确定问题，对2中所建立的待优化问题，采用贪婪算法计算针对不同目标应该选择的波束；
4. 假设如3，针对非短视下的调度问题，基于动态规划方法，编写Matlab仿真程序，给出小规模问题下的最优解；
5. 假设如3，进一步分析雷达波束调度问题，选取基策略，给出对应的Rollout算法，从理论上分析Rollout算法所能达到的性能及计算时间/空间复杂度，编写Matlab仿真程序，在算法性能、计算量等方面与短视策略、动态规划算法进行比较分析并给出算法结论。


## 论文组织

# 调度算法研究

## 动态规划算法
这个也是一个精确的算法。动态规划，简单说来就是知道一个状态，然后根据状态转移方程得到另外一个状态。根据这两个状态间的转换，递推获得最终解。
考虑这样一个问题，假如问题有n个阶段，每个阶段都有多个状态，不同阶段的状态数不必相同，一个阶段的一个状态可以得到下个阶段的所有状态中的几个。那我们要计算出最终阶段的状态数自然要经历之前每个阶段的某些状态。
如果下一步最优是从当前最优得到的，所以为了计算最终的最优值，只需要存储每一步的最优值即可，解决符合这种性质的问题的算法就变成了前文提到的贪婪算法。
如果一个阶段的最优无法用前一个阶段的最优得到呢？这时你需要保存的是之前每个阶段所经历的那个状态，根据这些信息才能计算出下一个状态！每个阶段的状态或许不多，但是每个状态都可以转移到下一阶段的多个状态，所以解的复杂度就是指数的，因此时间复杂度也是指数的。这种之前的选择会影响到下一步的选择的情况就叫做有后效性。
每个阶段的最优状态可以从之前某个阶段的某个或某些状态直接得到这个性质叫做最优子结构；

而不管之前这个状态是如何得到的这个性质叫做无后效性。
动态规划算法的设计可以分为如下四个步骤

1. 描述最优解的结构
2. 递归定义最优解的值
3. 按自底向上的方式计算最优解的值
4. 由计算出的结果构造最优解

## 贪婪算法
贪婪算法是一种短视算法，它会每次选择一个局部最优解，而不会考虑到该解对于今后的影响。一般来说，他的时间复杂度比较低，虽然贪婪算法并不从整体上最优加以考虑，导致它所作出的选择只是某种意义下的局部最优选择，这在工程实践中反而比较常用。
## rollout算法研究
rollout算法对各种动态和离散优化问题具有十分优异的表现。作为近似的动态规划算法，一个rollout算法，在每个决策阶段会通过以称为基策略的贪心策略模拟未来事件的方法来使得选择近似理想的方案。虽然在很多情况下rollout算法，可以保证执行情况和它们的基策略一样，已经出现了一些理论成果显示采用rollout算法，性能会有额外的改善。根据现有文献的随机模型，本文分析被称之为consecutive rollout和exhaustive rollout的两种rollout方法，这两者采用简单的贪婪算法作为基策略。我们分析这两种算法在普遍问题下的应用场景及算法流程，然后结合我们的实际问题，对这两种算法进行分析。
###consecutive rollout
 - 算法思路
 consecutive rollout算法的程序流程见下面，该算法输入为。。。，输出为。。。。，并调用贪婪算法作为其子程序。
 算法流程。。

  - 边界分析

###exhaustive rollout
 - 算法思路
在每次迭代中，用t索引，该算法考虑在现有的顺序一它计算由移动每个项目序列的前部，施加致盲贪婪算法得到的值的所有项目。然后，该算法具有最高估计值添加项（如果它存在）到溶液中。我们暗含的假设一致的打破平局方法，如该项目优先考虑以最低的指数。下一次迭代，然后用物品的其余序列前进。
考虑
 算法流程。

  - 边界分析



我们可以发现，对于 consecutive rollout 算法，他会依次对当前的是否选择情况进行一个基策略，看是选择还是不选择的结果会好一些，对于 exhaustive rollout 算法，它需要对每次剩余的结果进行穷举，然后把其中的某一项移动最前方，对该序列再次进行遍历，然后从其中选择一个最优的。
结合我们的实际情况，本文需要调度的是M个传感器的选择问题，每一步的选择情况都是固定的，是一个和M有关的表达式。可以很方便的使用 consecutive rollout 算法。而在传感器的选择方面，具有很强的时序相关性，时间序列无法随意改变，所以 exhaustive rollout 算法显然不适用于本文的情况。
另外本文采用的rollout算法，均为单次迭代算法，在今后的工作中我们可以考虑rollout算法的第二次迭代。一个相关的议题是仍考虑rollout算法只是第一次迭代，但具有较大的超前长度（如试图对所有项目进行了 exhaustive rollout，而不仅仅是单独的每个项）。

# 粒子滤波算法研究
## 基本算法
粒子滤波器是贝叶斯滤波器的一种可替代的非参数实现。就像histogram滤波器，粒子滤波器通过有限数量的参数逼近后验概率。但是，它们产生这些参数的方式以及状态空间的转移是不同的。粒子滤波器的核心思想是由一组根据这个后验概率得到的状态的随机样本来表示后验概率bel(xt)。粒子滤波用一组从这个分布得到的采样来表示这个分布，而不是由参数形式表示分布（例如表示一个正态分布概率密度的指数函数）。这样的表示是近似的，但它是非参数，因此它可以表示更广阔的分布比空间。

在粒子滤波器中，一个后验分布的采样称为粒子，由下式表示

X_t := x_t^([1]),x_t^([2]),...,x_t^([M])
每个粒子x_t^([m])(1≤m≤M)是在时间t状态的的具体实例，也就是，对于在真实世界时间t状态可以是在什么的一个假设。 在这里M表示

书78PR 90pdf页数
表4.3 粒子滤波算法，基于重要性采样的贝叶斯滤波器的变体。

粒子数。在实践中，粒子的数量M通常是一个大数，例如，M = 1000。在一些实现中，M是一个t或者其他和belief bel(x_t)有关量的函数。
粒子滤波器的重点是根据粒子集合X_t近似belief state bel(x_t)。理想的情况下，状态假设包括在粒子集合中的可能性应该正比于它的贝叶斯滤波后验概率bel(x_t)：

x_t^([m])~p(x_t|z_(1:t),u_(1:t))

由上式可知，一个状态空间的一个子区域由采样填充的越密集，真实的状态就跟可能落入这个区域。正如我们将在下面讨论，对于标准粒子滤波算法该特性仅在M近似于∞时成立。对于有限M，粒子是从具有略微不同的分布中得到。在实践中，这种差异是可以忽略的，只要粒子的数量不能太小（例如，M≥100）。

粒子滤波算法递归构建belief bel(x_t)根据早一个时间步长的belief
bel(x_(t-1))。因为belief是由粒子集合表示，这意味着粒子滤波器构造的粒子从XT-1到XT递归。粒子滤波器算法的最基本的变体可以参考<(￣3￣)>
表！ 。该算法的输入是
粒子集合XT-1，最近的控制UT和最近的量测ZT。该算法首先构造一个临时粒子集合ˉx，它类似于（但不等同）的bel(x_t)。它如下系统地处理输入粒子集合X_(t-1)中的每个粒子X [M]
1. 第4行基于粒子×〔米]叔1和控制UT在时间t产生一个假想状态。。。。 t。所得采样由m作为索引，这表明它是由XT-1的第m个粒子产生的。该步骤包括从下一状态分布p（XT | UT，XT-1）的采样。对于任意分布P（XT | UT，XT-1），没有从状态转移概率采样的统一公式。然而，生产实践中的许多主要分布具有有效的采样算法。步骤4中这组由迭代M次产生的粒子是bel(x_t)的滤波器的表示。
2，第5行计算每个粒子X [M]的重要性因子，记作W [M]。重要性因子用于合并观测ZT进入粒子集合。因此，重要性因子是粒子×〔米得到观测ZT的的概率。即，瓦特[米]
T = P（ZT | X [M] T）。如果我们的把W [M] T考虑为一个粒子的权重，那么加权粒子集合表示（近似地）贝叶斯滤波器后验概率bel(x_t)。
3.第8行至11行。这些行实现了所谓的重采样或重要性重采样。该算法对于临时集合ˉXT中的M个粒子判断是否需要。保留每个粒子的概率是由它的重要性权重给出。重采样将一个具有M个粒子的集合转换为另一个具有相同的大小的粒子集。通过将重要性权重纳入重采样过程中，粒子的分布因此而变化：在重采样步骤之前，他们根据bel(x_t)分布，经过重采样后，它们近似根据后验概率bel(x_t) η= P（ZT | X [M]T）bel(x_t)。事实上，所得的样本集合通常具有许多重复的。更重要的是没有被包括在XT中的粒子：那些于具有较低重要性权重的粒子。

重采样步骤具有强制粒子返回后验概率bel(x_t)的重要作用。实际上，另一种（通常比较差）版本的粒子滤波器将永远不会重采样，而是将维持每个粒子的重要性权重，权重被初始化为1然后根据下式更新：
W [米]T = P（ZT | X [M] t）的W [M] T-1（4.24）

这样的粒子滤波算法仍接近后验概率，但它的许多粒子将在后验概率很低的区域结束。其结果是，它需要更多的粒子，需要的数量的多少取决于后验概率的大小。重采样步骤是优胜劣汰的达尔文想法的概率实现：重新调整它的粒子集合到高后验概率状态空间的区域。通过这样做，它把滤波器算法的计算资源分配到最重要的状态空间所在区域。
## 重要性采样
用于粒子滤波器的推导，应当证明是有用的，下面以更详细地步骤讨论重采样。图4.2显示了采样步骤下的intuition。图4.2A示出了一个被称为目标分布的概率分布的密度函数f。我们想实现的是从f计算采样。然而，从f直接采样可能是不可能的。相反，我们可以从相关密度生成粒子，在图4.2B标记g。对应于概率密度g的分布称为(proposal distribution)建议分布。概率密度g为必须满足函数f（x）> 0意味着g（x）的> 0，使得对于从任何f可能生成的状态，根据g进行采样时，都可以有一个非零的概率来生成一个粒子。然而，所得到的粒子集合，在图4.2B的底部示出，是根据g而不是f分布的。特别地，对于任何一个区间范围。。。。（或更一般地，任何Borel集合A）粒子的经验计数落入一个收敛于A下对g的积分：
（4.25）
为了消除f和g之间的不同，粒子x^([m])是由商加权

（4.26）
p81 tu%%%%%%%%%

这由图4.2C表示：该图中的竖线表示重要性权重的大小。重要性权重是每个粒子的非归一化概率质量。特别地，我们有
4.27）
其中第一项作为用于所有重要性权重的正规化因子。换言之，尽管我们由密度g生成粒子，适当加权的粒子收敛于密度F。
具体的趋近设计对于集合A的一个积分。显然，一个粒子集合表示一个离散分布，而在我们的例子中f是连续的。由于这个原因，就没有可能与粒子集合相关联的密度。 因此，收敛是f的累积分布函数，而不是密度本身。重要抽样的一个很好的特性是如果在f（x）> 0时都有g（x）> 0它趋近于真密度。在大多数情况下，收敛律为O(1/sqrt(M))，其中M是采样的数目。恒定系数取决于f(s)和g(s)的相似度。
在粒子滤波器中，密度f对应于目标belief bel(x_t)。在此假设下，X_(t-1)中的粒子根据bel(x_t-1)分布，密度g对应于乘积分布：
P（XT | UT，XT-1）bel(x_t-1)  （4.28）
这个分布称为建议分布。 
## 公式化
为了获得粒子滤波器的数学表示，我们可以把粒子当作状态序列的采样
(4.29）
相应地修改该算法也很容易：只需追加粒子。。。到状态采样序列。。。。这种粒子滤波器计算在所有的状态序列上的后验概率：
bel（X0：T）= P（X0：T | U1：T，Z1：T）（4.30）
而不是belief bel(x_t)= P（XT | U1：T，Z1：T）。诚然，在所有状态序列上的空间是巨大的，并且用粒子覆盖它通常是显然不可行。然而，这不应阻止我们在这里，因为这个定义只是推导粒子滤波算法的手段。
根据文献xxx我们有，如下公式
（4.31）

验证初始条件是不重要的的，假设根据先验概率P（X0）采样获得我们的第一个粒子集。让我们假定粒子集在时刻t-1，根据bel（X0：T-1）分布。对于这个集合中的第m个粒子X [米]
0：T-1，在我们的算法的步骤4中产生的样本x [M] T是从建议分布产生的：
ρ（XT | XT-1，UT）bel（X0：T-1）=ρ（XT | XT-1，UT）P（X0：叔1 | Z0：T-1，U0：T-1）4.32）
（4.33）
常量η没有任何作用，因为重采样发生概率与正比于重要性权重。通过重采样概率正比于重要性权重的粒子，所得的粒子根据proposal与重要性权重的乘积分布
（4.34）
（请注意，常数系数η这里不同于（4.33）中的因子。）在表4.2的算法转换为从一个简单地观测得到，如果x [米]0：t为根据bel（X0：吨）分布，则状态采样x [米]t根据bel(x_t)分布。
正如我们下面讨论的，由于我们考虑到规一化常数，这个推导仅对于M→∞正确。然而，即使是有限的M，它也是有效的。
## 误差来源

# POMDP建模
现在让我们将焦点切换到部分可观的问题。在马尔科夫决策过程中只解决行动中的不确定性，但他们假设世界的状态可以有把握地确定。对于完全可观马尔可夫决策过程，我们设计了一个数值迭代算法用于控制随机域中的机器人。现在，我们将有兴趣在更一般的情况，其中的状态不是完全可观的。缺乏观测性是指机器人只能估计一个P后验分布在可能的世界状态，我们称之为belief。此设置也称为部分可观马尔可夫决策过程，或POMDPs。
在本章的其余部分要解决的核心问题是：对于POMDPs我们能否制定规划和控制算法？答案是肯定的，但具有前提。寻求最优策略的算法仅存在于有限的世界，其中状态空间，动作空间，观测空间，以及计划向量T均为有限的。不幸的是，这些精确的方法的计算是十分复杂的。对于更有趣的连续的情况下，最有名的算法都是近似的。
在MDPs中的中心更新方程如下
15。14
在POMDPs中，我们应用非常接近的想法，然而，状态并不是可观的。我们只可以得到states 的后验分布belief state b。因此对于POMDPs的计算方程如下
16.2
并且我们使用这个方程来生成控制
16.3
不幸的是，在belief空间计算价值要比在state空间复杂的多。belief是一个概率分布，因此，在POMDPs中的价值C_T是有关概率分布的函数，这就是问题所在。如果状态空间是有限的，由于belief space是所有状态空间的分布，所以belief state就是连续的。对于连续状态空间的问题，这个情况更加复杂，在这种情况下，belief space是一个无线维的连续体。除此之外，公式。。和公式。。对于所有的beliefs b进行积分。由于belief space的复杂度，积分并不容易精确计算或者有效地近似，
幸运的是，对于有限的情况，存在确切的解法。
本章安排如下，
第二节，讨论了一种更加普遍的同时考虑两种不确定性：动作不确定性和观测不确定性的调度算法。这种算法可以被应用于belief space的表示，在这种算法下的框架称作Partially Obeservable Markov Decision Processes(POMDPs)。这种算法可以估计不确定性，高效的获得信息，并且可以提高精确度在增加计算复杂度的情况下。
16.2有限环境
下面考虑一种十分重要的特殊情况，有限环境。在这里，我们假设我们有一个有限状态空间，在每个状态具有有限的动作，有限数量的观测，和有限计划时域.在这些情况下，最优值函数可以被精确计算，也即具有最优策略。然而这并不是对所有情况都很明显，即使状态空间是有限的，事实上可以有无限多的可能的belief。然而，在这一点，下面我们建立一个众所周知的结果，最优值函数是凸的并且由有限多个线性部分组成。因此，即使该值函数被定义在一个连续体，它可以在计算机中以浮点数的精度来表示。
说明性的例子
一个有限的世界的一个例子示于图16.1。这个具体的例子极端简单，其唯一的作用是在讨论一般的解决方案前帮助熟悉POMDPs的关键问题。我们注意到，图16.1具有四种状态，标记S1至S4中，两个动作，a1和a2，和两个观察，标记O1和O2。初始状态是随机抽取来自两个顶端状态，s1和s2。现在机器人给出一个选择：执动作作A1，这将具有高概率（但不总是）瞬间移动到相应的其他状态。另一种可选择方案，它可以执动作作A2，这导致在这两个底状态，S3或S4的概率中的一个的转变所示。在S3中，机器人将收到一个大的正收益，而在S4的收益为负。这两个state是终端的状态，也就是说，一旦进入任务就结束了。因此，机器人的目标是在状态S1时要执动作作A2。如果机器人知道它是在什么状态，执行任务是非常简单：简单地套用动作A1，直到状态是S1，然后应用动作A2。然而，机器人不知道其状态。相反，它可以感知两个观测O1和O2。不幸的是，这两个观测可能在这两种状态，虽然观察一比其他的概率不同，这取决于什么状态的机器人是由于机器人并不知道它的初始状态是S1或S2，它必须仔细跟踪过去的观测来计算它的belief。因此，关键的策略问题是：什么时候应该机器人尝试动作A2？具有多大的置信度去执动作作a2，需要多长时间来尝试这个动作？这些和其他问题将在下面进一步描述。
本节的目标是开发一种在有限状态和有限时域T的情况下来计算最优值函数的算法。让我们表示用T来表示计划时域和用S1,...,SN表示状态。利用状态空间的有限，

图16.1有限状态环境，用来说明价值迭代的belief空间。

我们注意到，belief B（S）由n个概率给出
圆周率= B（S= SI）（16.4）
其中i=1，。 。 。 ，N。
belief必须满足的条件
PI≥0
（16.5）
因为最后一个条件的，B（S）可通过n-1个参数p1,...,p_n而不是n个参数指定。剩余的概率的pn可以根据如下公式计算：
（16.6）。
因此，如果状态空间是有限的，并且大小为n的话，则belief是一个（N-1）维时域。
在我们的图16.1示出的例子中，可能出现的是由三个数值的概率值指定的belief，因为有四个状态。然而，操作A2将状态{S1，S2}和状态{S3，S4}分开。因此，该机器人可能遇到的唯一的不确定性是状态s1和s2或者状态S3和S4之间的的混淆。两者均可以由一个单一的数值的概率值来表示，由此，belief状态的唯一连续成分是一维的。这使得这个例子方便绘制值的函数。
现在，让我们专注于一个问题，我们是否可以计算出最优值函数，准确地在有限的领域最优策略。如果没有，我们可能会被迫以接近它。起初，人们可能会得出这样的结论计算最优值函数是不可能的，由于这一事实，即belief空间是连续的。但是，我们观察到的有限的世界，值函数有一个特殊的形状：它是由有限多个线性件。这使得能够计算在有限时间内的最优值函数。我们还注意到，价值函数是凸和连续这后两种特性也适用于最优值函数在连续状态空间和无限的规划视野。
我们开始的belief状态B的直接回报我们的考虑。回想一下，b的收益是由下概率分布B的收益C中的给定期望：
C（二）=？ C（S）B（S）DS（16.7）
使用使得b唯一地由概率p1指定的事实。 。 。 ，PN，我们可以写C（B）= C（P1，。，PN）=
ñ
？
C（SI）PI
I = 1
（16.8），它确实是线性p1中，。 。 。 ，PN。
图16.2期望收益C（B）为belief参数P3的功能，假设机器人无论是在状态S3或S4。
有趣的是绘制函数c（b）在置信度分布在两种状态S3和S4在我们的例子中，只有国家与非零回报。在状态S3回报
是100，而在状态S4的回报是-100。图16.2a示出了函数c（b）为相信通过？0,0，第3页，1-P3限定的子空间？这是一种belief的空间，放置在美国S3和S4所有的可能性。这里的期望C（b）的绘制为
函数的概率P3的。显然，如果P3 =0时，环境状态是S4，并回报将是C（S4）= -100。另一方面，如果P3 =1，环境的
状态是S3和回报将是C（S3）= 100。在此期间，期望是线性的，从而导致在图16.2a所示的曲线图。
这种考虑使我们能够计算价值函数规划时域T = 1。从现在开始，我们将只考虑belief空间的地方的子集
在两种状态S1和S2所有的可能性。这种belief空间是由一个单一的参数参数化，p1的，因为P2 = 1 - P1和P3 = P4 = 0的值函数C1（二，a1）中是恒定的零动作A1：
C1（二，A1）= 0（16.9）
因为无论机器人的真实状态，动作A1不会导致它的状态，使得它收到非零的回报。该值函数C1（B，A1）如果绘制如图16.2b。图片变得更有趣动作A2。如果环境的状态是S1，这个动作将导致90％的几率状态S3，其中机器人将收到的100的收益有10％的概率会结束在状态S4，在那里它会回报
-100。因此，在状态S1的预期收益为0.9·100 + 0.1·（-100）= 80。类似的说法，状态S2的预期收益为-80。在这两者之间，期望是线性的，产生的价值函数
C1（B，A2）=γ（80p1 -80p2）= 72p1 -72p2（16.10）
这里我们使用的折现因子γ= 0.9。这个函数表示在图16.2c，该类型的belief？P1，1-P1，0，0℃。
那么，什么是正确的动作选择策略？继最大化预计这将对收益的理由，最好的动作取决于我们目前的belief，假设它准确
反映了我们对现实世界的知识。如果概率P1≥0.5，最佳动作将A2，因为我们期望正回报。对于值小于0.5，最佳动作将A1，因为它避免了与动作A2相关的负面预期收益。对应的价值功能的动作特定值功能最大：
C1（B）=最大为C1（B，A）
=最大值{0; 72p1-72p2}（16.11）
该值函数和动作选择了相应的策略由图16.2d，从而最大程度地说明了两个线性成分的固体图所示
由虚线。我们注意到，这个数值函数不是线性的任何更长的时间。相反，它是分段线性和凸出的。非线性产生的事实，不同的操作是最佳的belief空间的不同部分。
从图16.2d的价值功能，我们可以得出结论，beliefP1 <0.5是没有办法获得的收益大于0？当然，答案是否定的。该
值函数C1（b）是仅最适合于时域T = 1。对于较大的视野，也能够先执动作作A1，接着动作A2。执动作作A1具有两个有益效果：首先，它有助于我们估计当前状态更好，由于这样的事实，在我们可以感知，并且第二，以高概率从s1至s2的，或反之亦然改变状态。因此，一个良好的策略可能会执行的动​​作A1，直到我们有理由相信，机器人的状态是S1。然后，机器人应执行的动作A2。
为了使这个比较正规的，让我们得出的最优值函数的时域T = 2，假设我们执行的动作A1。然后，两件事情可能发生：我们要么遵守O1 O2或。让我们先假设我们观察到O1。那么新的belief状态可以使用贝叶斯滤波器来计算：
2个P？ 1 =η1P（O 1？|？第1条）
？
P（S 1 |？A1，SI）PI
I = 1
=η10.7（0.1p1 + 0.8p2）=η1（0.07p1 + 0.56p2）
（16.12）
其中，η1是贝叶斯法则的正规化。同样，我们得到的是S2后概率：
2 p? 2 = η1 P(o? 1|s? 2)
?
P(s? 2|a1, si) pi
i=1
= η1 0.4(0.9p1 +0.2p2) = η1 (0.36p1 +0.08p2)
(16.13) Since we know that p? 1 + p? 2 = 1—after all, when executing action a1 the state will
either be s1 or s2—we obtain for η1: η1 =
1 0.07p1 +0.56p2 +0.36p1 +0.08p2 = 1 0.43p1 +0.64p2 (16.14)
我们也注意到，变量η1是一个有用的概率：这是执行的动作A1（无论后状态的）后观察O1的概率。
我们现在有表情刻画因为我们执行AC-后belief
A1化观察O1。那么，什么是这个国家的belief价值？答案是转播tained通过插入新的belief到价值函数C1（b）的公式定义（16.11），和γ贴现的结果：
C2（B，A1，01）=γ最大值{0; 72P？ 1 -72p？ 2}
= 0.9最大值{0; 72η1（0.36p1 + 0.08p2）-72η1（0.43p1 + 0.64p2）} = 0.9最大值{0; 72η1（0.36p1 + 0.08p2 -0.43p1 -0.64p2）} = 0.9最大值{0; 72η1（-0.07p1 -0.56p2）} = 0.9最大值{0; η1（-5.04p1 -40.32p2）}
（16.15）
我们现在移动的因素η1出来的最大化和移动0.9里面，并取得：C2（B，A1，01）=η1最大值{0; -4.563p1 -36.288p2}
（16.16），其是一个分段线性的，凸的函数的beliefb的参数。
推导的执行动作A1后观察O2是完全类似。特别是，我们得到的后验
p?
1 = η2 0.3(0.1p1 +0.8p2) = η2 (0.03p1 +0.24p2)
p?
2 = η2 0.6(0.9p1 +0.2p2) = η2 (0.54p1 +0.12p2)
with the normalizer η2 =
1 0.57p1 +0.36p2
The corresponding value is C2(b, a1, o1) = γ max{0 ; 72p?
1 −72p? 2} = η2 max{0 ; −33.048p1 +7.776p2} (16.17) (16.18) (16.19)
顺便说一句，我们也注意到，η-1 1 +η-1 2 = 1。这直接遵循这样的事实
在贝叶斯正规化过滤是观察概率ηI=
1 P（O I |？A1，B）（16.20）和事实恰好有两种可能的观测我们的例子中，O1和O2。
现在让我们计算值C2（二，a1）中，这是在执动作作A1的预期值。显然，该值是值C2（二，A1，01）和C2（二，A1，O2），通过实际观察O1和O2分别的概率加权的混合物。放入数学符号，我们有
C2（二，A1）= 2
？
C2（B，A1，OI）P（OI | A1，B）
I = 1
（16.21）
术语C 2（二，A1，OI）已经如上所定义。概率P（OI | A1，B）执动作作A1之后观察OI是η-1
我。因此，我们有所有成分
计算所需值C2（二，a1）中：C2（二，A1）=η-1
1η1最大值{0; -4.563p1 -36.288p2} +η-1
2η2最大值{0; -33.048p1 + 7.776p2}（16.22）= MAX {0; -4.563p1 -36.288p2} +最大{0; -33.048p1 + 7.776p2}
该表达式可以被重新表示为四个线性函数的最大值：C2（二，A1）=最大{0; -4.563p1-36.288p2;
（16.23）-33.048p1+7.776p2; -37.611p1-28.512p2}
图16.2e表示这四个线性函数的belief间隔p1，1 - P 2，0，0℃。该值函数C2（B，A1）是这四个线性函数的最大值。由于是
容易被看到的那样，二者的这些功能是足以定义最大;其他
2顷在整个频谱较小。这使我们能够改写的C2（二，a1）中为最大值的仅有的两个方面，而不是四个：C2（二，A1）=最大{0; -33.048p1+7.776p2}
（16.24）
最后，我们要确定的值C2（b）中，这是下面两个条件中的最大值：
C2（B）=最大{C2（二，a1）中，C 2（二，A2）}（16.25）
正如很容易地验证，则第二项的最大化，C2（二，a2）中，是完全与上述相同的用于规划时域T = 1：
C2（二，A2）= C1（二，A2）= 72p1 -72p2
因此，我们获得（16.24）和（16.26）：
（16.26）
C2（B）=最大{0; -33.048p1 + 7.776p2; 72p1 -72p2}
（16.27）这个函数是最优值函数规划时域T.图？图
C2（b）和其组成部分的belief子空间P1，1 - P 2，0，0℃。正如很容易看到的，函数是分段线性和凸出的。特别是，它包括三个
线性件。对于在最左边的两个片belief（​​即P1 <0.5），A1是最佳的动作。对于P1 = 0.5，这两个动作都是一样的好。对应于最右边的线性片的belief，即用P1> 0.5的belief，有最佳的动作A2。如果a1为最优函数，下一个动作取决于在belief空间的初始点和在观察。
价值函数C 2（b）是仅最适合于时域2。但是，我们的分析示出了几个重要的观点。
首先，最优值函数的任何有限的视距是连续的，分段线性，凸。每个线性一块对应于一个不同的操作选择在将来某个时候，或者，可以进行不同的观察。价值函数的凸性表示相当直观的观察，即明知是AL-方面都优于不知道。给定两个belief状态B和B?,的belief状态的混合值大于或等于该混合belief状态的值，对于某些混合参数β0≤β≤1：
βC（二）+（1-β）C（二？）≥-C（βB+（1-β）B'）（16.28）
二，直线条的数量可以极大地增长，特别是如果一个人不注重线性函数变得过时。在我们的玩具例子中，两个出的四个直线约束限定的C2（二，a1）的是没有必要。 EF-ficiently实施POMDPs的“诀窍”在于早识别过时线性函数
如可能，从而使计算的价值函数时，没有计算被浪费。不幸的是，即使我们仔细排除一切不必要的线性约束，线性函数的num- BER仍然可以增长得非常迅速。这对对有限POMDPs的精确值迭代解决方案固有的扩展限制。

16.2.2belief空间中的价值迭代
在上一节展示来如何计算有限的价值函数。现在让我们回到在belief空间上价值迭代的一般问题。特别是，在介绍这一章中，我们指出了价值函数，我们简要地重申这里的基本更新方程：
（16.29）
在这个和下面的章节中，我们将开发一个可在数字计算机上实现的通用的在belief空间计算价值迭代的算法。首先，我们注意到，公式（16.2）提出对所有的belief进行积分，其中每一个belief是一个概率分布。如果状态空间是有限的，并且状态数为n，所有的概率分布的空间是具有维数n-1的连续体。我们注意到，为了指定在n个离散事件的概率分布n-1个数值是必需的（第n参数可以省略，因为概率加起来为1）。如果状态空间是连续的，belief空间具有无限多的维度。因此，对所有的belief的积分似乎是计算艰巨的任务。然而，我们可以通过重新制定的问题，并对所有观测进行积分来代替。
让我们看看条件概率P（B|？A，B），其中规定了在分配后的belief B'给定一个belief b和动作a。如果只有b和a是已知的，该后验belief B'不是唯一的，而且P（B |？的a，b）是belief的一个真正的概率分布。但是，如果如果我们也知道测量Ò，执行动作时，后验B'是独特的，P（B|？的a，b）是一个point-mass分布的退化。从动作执行之前的beliefb，动作a，和随后的观测O 3，贝叶斯滤波器计算一个单一的准确的后验概率belief B'。因此，我们得出结论，只要我们知道O 3，对所有的belief的积分会退化。
这种洞察力可以通过重新表达P利用

（16.30）

其中，P（B |？的a，b，邻）是由贝叶斯滤波器计算的point-mass 分布将积分代入价值迭代的定义（16.2），我们得到
（16.31）
内积分
（16.32）
仅包含一个非零项。这其中b‘是由B，A和O的分布通过贝叶斯滤波器计算得到。让我们用B（B，A，邻？）来表示这个分布，那么有
（16.33）
我们注意到，正规化分母，P（O|？A，B），是值更新方程（16.31）的一个因素。因此，将B（B，A，O 2）代入（16.31）消除了这一量，从而导致了值迭代算法的递归描述：
（16.34）
这种形式是比在（16.2）更加方便，因为它只需要对所有可能的测量O 2，而不是所有可能的belief分布b？进行积分，这种转变是派生此一章所有值迭代算法的一个重要观点。事实上，它是含蓄地用在上面的例子中，在一个新的价值函数是由有限多个分段线性函数混合获得的。
下面，很容易根据积分获得最大化价值的动作。因此，我们会注意到（16.34）可以改写为下列两个等式：
（16.35）
（16.36）
在这里，CT（B，A）是时域T对于假设下一个动作a的belief b的函数。

16.2.3计算价值函数
在我们的例子中，最优值函数是分段线性和凸的。这种情况，对于一般的有限情况均满足。对于所有具有有限时域的有限POMDPs的最优值函数都是分段线性和凸的。分段线性度意味着价值函数CT由线性函数的集合表示。如果CT是线性的，它可以通过一组系数。。。CT来表示

（16.37）
其中，像往常一样，P1。 。 。 ，PN是一种belief分布的参数。在我们的示例中，分段线性和凸值函数CT（B）可以通过K个线性函数集合的最大值表示

（16.38）
其中，CT K，1，。 。 。 ，CT K，n分别表示第k个线性函数的参数，以及K表示线性片段的数量。一组有限的线性函数的最大值事实上是凸的分段线性的函数。

在值迭代，初始值函数由下式给出
C0 = 0（16.39）
我们注意到，这个数值函数是线性的，因此，它也是分段线性和凸。这种分配建立了递归值迭代算法的基础。
我们现在衍生除一个计算值函数CT（b）的递归方程。该公式假定上一步的值函数CT-1（b），由一个分段线性函数如上规定表示。作为派生的一部分，我们将假设CT-1（b）是分段线性和凸的，CT（b）是也分段线性和凸出的。下面将证明具有有限时域的值函数是分段线性和凸的。
方程（16.35）和（16.36）定义如下：
（16.40）
（16.41）
在有限的空间中，所有的积分可以通过有限和取代，我们得到：

（16.43）
belief B（？B，A，O）使用下面的表达式获得，该式为用和来替换（16.33）中的积分所得到：

B（？B，A，O）（S？）= P（O|？A，B）P（O|？S）？P（S|？A，S）B（S）1秒（16.44）

如果belief B由参数{p1，。。。 ，PN}表示，那么belief b’中的第i个参数由下式计算得到

（16.45）

为了计算价值函数CT（B，A），我们将现在的条件c（B（B，A，O 2））和CT-1（B（B，A，O 2） 中衍生出更多实用的表达式中），从公式（16.42）开始，具有参数B（B，A，O 2）={对？
1，。 。 。页？ N}，我们从。。得到了

（16.46）
代入（16.45），我们得到
（16.47）
由于P（邻？|的a，b）不依赖于i，后者变换是合法的。这种形式仍然包含一个难以计算的表达式：P（O| A，B？）。然而，在有限的情况下这个表达式可以被消掉。因此，它不必被任何进一步考虑。
在（16.42）中对量CT-1（B（B，A，O 2））的推导类似于上面C（B（B，A，O 2））。特别是，我们不得不用唯一的分段线性和凸的值函数CT-1取代直接回报函数c。然后可以得到

（16.48）
。如上所述，我们代入后验概率belief B（见（16.45）），并获得：

（16.49）
其中P（ω0| A，B）-1不依赖于i或K，故可移出求和和最大化部分，得到：
（16.50）
该表达式也是分段线性和凸的可能并不明显。然而，对该式进行整理得到下式：

（16.51）

在这种形式下，很容易验证，对于任何的固定值k，使其最大化的参数是确实线性的参数PJ。因而，这些线性段的最大值为分段线性和凸的。此外，线性段的数量是有限的。
现在让我们回到本节中的主要问题，即计算CT（B，A）和时域T的值函数。我们简要地重申方程（16.42）：
16.52）

让我们首先用公式（16.47）和（16.50）计算总和C（B（B，A，O）？）+ CT-1（B（B，A，O）？），：

(16.53) 
经过一些整理得到，
(16.54)

将这个表达式代入公式16.52
(16.55)
我们注意到，项P（邻|的a，b？）同时出现在在此表达式的分子和分母中从而抵消了，得到：
（16.56）

这个消除是仅对有限POMDP满足的。所需的价值函数通过对于所有动作最大化CT（B，A）：
CT（B）得到

（16.57）
很容易验证，在belief参数为P1，。 。 。 ，PN的情况CT（b）是确实分段线性和凸的。具体地，对于每个固定k（16.56）的大括​​号内的量是线性的。这些k个线性函数的最大值是一个分段线性函数，包括最多有K段。分段线性的凸函数的总和为再次分段线性和凸的。因此，因此对于所有观测Ø？的乘积的总和再次产生一个分段线性凸函数。最后，对于在方程（16.57）中的所有动作的最大值也是分段线性的凸函数。因此，我们的结论是在任何有限时域T，CT（b）是分段线性和凸出的。
# 仿真实验结果
